{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.util import compounding, minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SpacyTokenTransformer(TransformerMixin):\n",
    "    __symbols = set(\"!$%^&*()_+|~-=`{}[]:\\\";'<>?,./-\")\n",
    "\n",
    "    def transform(self, X: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        f = np.vectorize(\n",
    "            SpacyTokenTransformer.transform_to_tokens, otypes=[np.object]\n",
    "        )\n",
    "        X_tokenized = f(X)\n",
    "\n",
    "        return X_tokenized\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_to_tokens(text: np.str) -> List[str]:\n",
    "        str_text = str(text)\n",
    "        doc: Doc = nlp(str_text)\n",
    "        tokens: List[str] = []\n",
    "        tok: Token\n",
    "        for tok in doc:\n",
    "            clean_token: str\n",
    "            if tok.like_url:\n",
    "                clean_token = \"URL\"\n",
    "            else:\n",
    "                clean_token = (\n",
    "                    tok.lemma_.strip().lower()\n",
    "                )  # if tok.lemma_ != '-PRON-' else tok.lower_\n",
    "                if (\n",
    "                    len(clean_token) < 1\n",
    "                    or clean_token in SpacyTokenTransformer.__symbols\n",
    "                    or clean_token in STOP_WORDS\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "            tokens.append(clean_token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    __uplus_pattern = re.compile(\"\\<[uU]\\+(?P<digit>[a-zA-Z0-9]+)\\>\")\n",
    "    __markup_link_pattern = re.compile(\"\\[(.*)\\]\\((.*)\\)\")\n",
    "\n",
    "    def transform(self, X: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        f = np.vectorize(CleanTextTransformer.transform_clean_text)\n",
    "        X_clean = f(X)\n",
    "\n",
    "        return X_clean\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_clean_text(raw_text: str):\n",
    "        try:\n",
    "            decoded = raw_text.encode(\"ISO-8859-1\").decode(\"utf-8\")\n",
    "        except:\n",
    "            decoded = raw_text.encode(\"ISO-8859-1\").decode(\"cp1252\")\n",
    "\n",
    "        html_unescaped = html.escape(decoded)\n",
    "        html_unescaped = re.sub(r\"\\r\\n\", \" \", html_unescaped)\n",
    "        html_unescaped = re.sub(r\"\\r\\r\\n\", \" \", html_unescaped)\n",
    "        html_unescaped = re.sub(r\"\\r\", \" \", html_unescaped)\n",
    "        html_unescaped = html_unescaped.replace(\"&gt;\", \" > \")\n",
    "        html_unescaped = html_unescaped.replace(\"&lt;\", \" < \")\n",
    "        html_unescaped = html_unescaped.replace(\"--\", \" - \")\n",
    "        html_unescaped = CleanTextTransformer.__uplus_pattern.sub(\n",
    "            \" U\\g<digit>\", html_unescaped\n",
    "        )\n",
    "        html_unescaped = CleanTextTransformer.__markup_link_pattern.sub(\n",
    "            \" \\1 \\2\", html_unescaped\n",
    "        )\n",
    "        html_unescaped = html_unescaped.replace(\"\\\\\", \"\")\n",
    "\n",
    "        return html_unescaped\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    __slots__ = \"_html_cleaner\", \"_tokenizer\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._html_cleaner = CleanTextTransformer()\n",
    "        self._tokenizer = SpacyTokenTransformer()\n",
    "\n",
    "    def clean_and_tokenize(\n",
    "        self, txt: Union[str, List[str], np.ndarray]\n",
    "    ) -> np.ndarray:\n",
    "        if isinstance(txt, str):\n",
    "            txt = [txt]\n",
    "\n",
    "        if isinstance(txt, list):\n",
    "            txt = np.array(txt, dtype=np.object)\n",
    "\n",
    "        if not (isinstance(txt, np.ndarray) and txt.dtype == np.object):\n",
    "            raise ValueError(\n",
    "                \"Input `txt` must be a string, list of strings, or numpy array of type object\"\n",
    "            )\n",
    "\n",
    "        txt_processed = self._html_cleaner.transform(txt)\n",
    "        txt_processed = self._tokenizer.transform(txt_processed)\n",
    "\n",
    "        return txt_processed\n",
    "\n",
    "    def clean_single(self, txt: str) -> str:\n",
    "        return self._html_cleaner.transform_clean_text(txt)\n",
    "\n",
    "    def tokenize_single(self, clean_text: str) -> List[str]:\n",
    "        return self._tokenizer.transform_to_tokens(clean_text)\n",
    "\n",
    "    def clean_and_tokenize_single(self, txt: str) -> List[str]:\n",
    "        txt_proc = self.clean_single(txt)\n",
    "        txt_proc = self.tokenize_single(txt)\n",
    "\n",
    "        return txt_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPth = \"../data/reddit_200k_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\",\"body\",\"score.x\",\"parent_id.x\",\"id\",\"created_utc.x\",\"retrieved_on\",\"REMOVED\"\n",
      "\"1\",\"I've always been taught it emerged from the earth after an impace. That is why it has similar elemental distribution to earth\",2,\"t3_81u15i\",\"dv551g6\",1520121101,1524782256,FALSE\n",
      "\"2\",\"As an ECE, my first feeling as \"\"HEY THAT'S NOT-\"\" and then I thought about all the times my co-workers couldn't even write a simple message in our communication book without making mistakes. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 {dataPth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['retrieved_on', 'parent_idx', 'created_utc.x', 'id', 'score_x']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cols = [\n",
    "    \"body\",\n",
    "    \"score_x\",\n",
    "    \"parent_idx\",\n",
    "    \"id\",\n",
    "    \"created_utc.x\",\n",
    "    \"retrieved_on\",\n",
    "    \"removed\",\n",
    "]\n",
    "\n",
    "TEXT_COL = \"body\"\n",
    "LABEL_COL = \"removed\"\n",
    "\n",
    "to_remove = list(set(df_cols) - {TEXT_COL, LABEL_COL})\n",
    "\n",
    "to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(dataPth, names=df_cols, skiprows=1, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# df.drop(columns=to_remove, inplace=True)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = pd.read_feather(\"reddit_200k_train.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've always been taught it emerged from the ea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday: Drug companies stock dives on good new...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i learned that all hybrids are unfertile i won...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well i was wanting to get wasted tonight.  Not...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  removed\n",
       "0  I've always been taught it emerged from the ea...    False\n",
       "1  As an ECE, my first feeling as \"HEY THAT'S NOT...     True\n",
       "2  Monday: Drug companies stock dives on good new...     True\n",
       "3  i learned that all hybrids are unfertile i won...    False\n",
       "4  Well i was wanting to get wasted tonight.  Not...    False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"body\"].values\n",
    "y = df[\"removed\"].values.astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in enumerate(X):\n",
    "    X_clean[i] = CleanTextTransformer.transform_clean_text(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [{\"REMOVED\": l, \"NOTREMOVED\": not l} for l in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(\n",
    "    X_clean, cats, stratify=y, test_size=0.1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy TextCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"arch\": \"simple_cnn\",\n",
    "    \"epochs\": 7,\n",
    "    \"batch_args\": (64, 512, 1.001),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if label == \"NEGATIVE\":\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_nlp = spacy.blank(\"en\")\n",
    "\n",
    "cat_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcat = cat_nlp.create_pipe(\n",
    "    \"textcat\",\n",
    "    config={\"exclusive_classes\": True, \"architecture\": hyperparams[\"arch\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_nlp.add_pipe(textcat, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['textcat']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcat.add_label(\"REMOVED\")\n",
    "textcat.add_label(\"NOTREMOVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(trainX, [{\"cats\": cats} for cats in trainY]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I thought Asian people were more likely to be lactose intolerant?',\n",
       " {'cats': {'REMOVED': False, 'NOTREMOVED': True}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(volfy/testing)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj = neptune.init(\"volfy/testing\", api_token=os.getenv(\"NEPTUNE_API_TOKEN\"))\n",
    "\n",
    "proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = compounding(*hyperparams[\"batch_args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = cat_nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/volfy/testing/e/TES-20\n"
     ]
    }
   ],
   "source": [
    "exp = proj.create_experiment(\n",
    "    name=f\"spacy_textcat_{hyperparams['arch']}_ep{hyperparams['epochs']}\",\n",
    "    params=hyperparams,\n",
    "    tags=[\n",
    "        \"spacy\",\n",
    "        \"textcat\",\n",
    "        \"v2.3\",\n",
    "        str(hyperparams[\"arch\"]),\n",
    "        str(hyperparams[\"epochs\"]),\n",
    "    ],\n",
    "    notebook_id=\"0c3b24b0-cc21-4eb9-a5d8-ce2c73c6e8c7\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH\tLOSS \t  P  \t  R  \t  F  \tTIME \n",
      "1\t0.048\t0.685\t0.685\t0.685\t241.325 s\n",
      "2\t0.003\t0.691\t0.691\t0.691\t232.485 s\n",
      "3\t0.001\t0.692\t0.692\t0.692\t227.624 s\n",
      "4\t0.000\t0.691\t0.691\t0.691\t218.884 s\n",
      "5\t0.000\t0.691\t0.691\t0.691\t227.564 s\n",
      "6\t0.000\t0.689\t0.689\t0.689\t225.327 s\n",
      "7\t0.000\t0.687\t0.687\t0.687\t223.214 s\n"
     ]
    }
   ],
   "source": [
    "heading = \"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\n",
    "    \"EPOCH\", \"LOSS\", \"P\", \"R\", \"F\", \"TIME\"\n",
    ")\n",
    "print(heading)\n",
    "exp.log_text(\"train_log\", heading)\n",
    "for i in range(hyperparams[\"epochs\"]):\n",
    "    losses = {}\n",
    "\n",
    "    start = datetime.now()\n",
    "    random.shuffle(train_data)\n",
    "    batches = minibatch(train_data, size=batch_sizes)\n",
    "\n",
    "    for batch in batches:\n",
    "        txts, lbls = zip(*batch)\n",
    "        cat_nlp.update(txts, lbls, sgd=opt, drop=0.2, losses=losses)\n",
    "\n",
    "    with textcat.model.use_params(opt.averages):\n",
    "        scores = evaluate(cat_nlp.tokenizer, textcat, testX, testY)\n",
    "\n",
    "    taken = (datetime.now() - start).total_seconds()\n",
    "\n",
    "    exp.log_metric(\"loss\", losses[\"textcat\"])\n",
    "    exp.log_metric(\"precision\", scores[\"textcat_p\"])\n",
    "    exp.log_metric(\"recall\", scores[\"textcat_r\"])\n",
    "    exp.log_metric(\"f-1_score\", scores[\"textcat_f\"])\n",
    "    exp.log_metric(\"time\", taken)\n",
    "\n",
    "    to_print = \"{0}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}\\t{5:.3f} s\".format(  # print a simple table\n",
    "        i + 1,\n",
    "        losses[\"textcat\"],\n",
    "        scores[\"textcat_p\"],\n",
    "        scores[\"textcat_r\"],\n",
    "        scores[\"textcat_f\"],\n",
    "        taken,\n",
    "    )\n",
    "    exp.log_text(\"train_log\", to_print)\n",
    "    print(to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.set_property(\"arch\", hyperparams[\"arch\"].upper());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.empty_like(testY, dtype=np.bool)\n",
    "y_pred = np.empty_like(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cats in enumerate(testY):\n",
    "    y_true[i] = cats[\"REMOVED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(cat_nlp.pipe(testX)):\n",
    "    y_pred[i] = doc.cats[\"REMOVED\"] > doc.cats[\"NOTREMOVED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6871008177639826"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6575313354264942"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.log_text(\"auc\", f\"{auc:.5f}\")\n",
    "exp.log_text(\"acc\", f\"{acc*100:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.set_property(\"auc\", f\"{auc:.5f}\")\n",
    "exp.set_property(\"acc\", f\"{acc*100:.5f}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spacy_simple_cnn_ep7.ml'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nm = f\"spacy_{hyperparams['arch']}_ep{hyperparams['epochs']}.ml\"\n",
    "\n",
    "nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_nlp.to_disk(nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: spacy_simple_cnn_ep7.ml/meta.json (deflated 50%)\n",
      "  adding: spacy_simple_cnn_ep7.ml/textcat/ (stored 0%)\n",
      "  adding: spacy_simple_cnn_ep7.ml/tokenizer (deflated 84%)\n",
      "  adding: spacy_simple_cnn_ep7.ml/vocab/ (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "!zip -9 {nm}.zip {nm}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.log_artifact(nm + \".zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Hyperparams logged.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.log_hyperparams(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Metrics logged.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.log_metrics({\"auc\": auc, \"accuracy\": acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"volf52/spacy-text-classification\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional outputs...\u001b[0m\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/volf52/spacy-text-classification\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/volf52/spacy-text-classification'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(\n",
    "    filename=\"spacy_model.ipynb\",\n",
    "    environment=None,\n",
    "    outputs=[nm + \".zip\"],\n",
    "    message=f\"{hyperparams['arch']} Ep{hyperparams['epochs']} training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_NLP_CV",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "neptune": {
   "notebookId": "0c3b24b0-cc21-4eb9-a5d8-ce2c73c6e8c7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
